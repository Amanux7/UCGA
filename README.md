# Unified Cognitive Graph Architecture (UCGA)

> **A novel graph-based approach to Artificial General Intelligence**

[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)
[![Python 3.8+](https://img.shields.io/badge/Python-3.8+-green.svg)](https://python.org)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org)
[![React Visualizer](https://img.shields.io/badge/UI-React%20%7C%20Tailwind-61DAFB.svg)](landing-page/)

**Author:** Aman Singh â€” Founder, UCGA Research Initiative

---

## ðŸŒŸ Abstract

This repository introduces the **Unified Cognitive Graph Architecture (UCGA)**, a novel graph-native cognitive framework designed to address fundamental limitations of traditional neural architectures in reasoning, memory integration, and adaptive intelligence. UCGA models intelligence as a recursive interaction between specialised cognitive nodes connected through adaptive weighted edges, incorporating persistent attention-based memory, self-evaluation, and conditional error correction.

Unlike traditional architectures that process information uniformly through stacked layers, UCGA explicitly separates cognitive functions into distinct, differentiable nodesâ€”bridging the gap between narrow deep learning systems and the flexible, self-correcting cognition observed in biological intelligence.

### Key Innovations (Breakthroughs)

- **AGI Architecture Layers:** 7 core intelligence layers (Perception, Cognitive Graph, Working Memory, Reasoning, Global Workspace, Executive Controller, Action) connected as a dynamic cognitive system.
- **Recursive Cognitive Refinement:** A cognitive loop that iterates *T* times, progressively refining understanding and "thinking longer" about hard problems.
- **Persistent Differentiable Memory:** Attention-based cross-episode memory bank retaining long-term cognitive store rather than just within-window attention.
- **Self-Evaluation & Correction:** A built-in critic that triggers corrective feedback loops when confidence drops below learned thresholds.
- **Non-saturating Cognition:** Discovered that replacing `tanh` with `GELU + LayerNorm` completely unblocks gradient flow across deep recursive steps, preventing vanishing gradients in recurrent graph traversals.
- **Multimodal Learning:** Empirically validated on real-world benchmarks achieving **90.5%** accuracy on AG News (NLP) and **60.0%** on CIFAR-10 (Vision), establishing broad generalisation.

---

## ðŸš€ Complete Development History (From Inception to Current)

UCGA has been developed through structured research phases, systematically expanding its cognitive capabilities from basic recursive loops to a fully multi-modal, agentic cognitive system.

### Phase 0 â€” Foundation (Initial Setup)
- **Architecture Core:** Designed the `CognitiveNode` base class and the 9-node cognitive graph wired together in `UCGAModel`.
- **Memory Implementation:** Built `PersistentMemory` â€” a differentiable external memory bank with attention-based read/write.
- **Gradient Stabilization:** Replaced traditional `tanh` activations with **`GELU + LayerNorm`** to prevent gradient saturation across deep recursive steps.

### Phase 1 â€” Recursion Utility Experiments
- **Hypothesis:** Does iterating `T` times genuinely help reasoning?
- **Polynomial Regression:** Demonstrated that `T=3` cognitive steps outperform `T=1` on function approximation tasks.
- **Algorithmic Sorting:** Proved that `T=3` improves structural re-ordering of sequences.
- **Multi-hop Reasoning (bAbI):** Showed that additional cognitive steps drastically improve memory-assisted chain-of-thought accuracy on synthetic reasoning tasks.

### Phase 2 â€” ReasoningNode Upgrade (MLP â†’ Transformer)
- **Module Upgrade:** Replaced the flat MLP-based reasoning with a new `TransformerReasoningNode`.
- **Mechanism:** Implemented Multi-head self-attention (4 heads) over concatenated `[perception_vector, memory_vector]` to model relational structure.
- **Architecture Support:** Allowed dynamic toggle between `reasoning_type = "mlp"` and `"transformer"` within the cognitive loop.

### Phase 3 â€” Comparative Experiment (MLP vs Transformer)
- **Validation:** Wrote `experiment_transformer_vs_mlp.py` to compare all 4 variants (`mlp/T=1`, `mlp/T=3`, `transformer/T=1`, `transformer/T=3`).
- **Results:** Transformer models excelled on Parity (binary XOR classification) and Sorting. Validated that temporal recurrence (`T=3`) yields superior test performance, state norms, and gradient norms across both architectures.

### Phase 4 â€” Real Dataset Training & Encoders
- **Vision:** Added `ImageEncoder` (CNN) and successfully trained on **CIFAR-10** image classification.
- **Language:** Added `TextEncoder` and `TransformerTextEncoder` (BoW and sequential), achieving high performance on **AG News** text classification and SST-2 sentiment analysis.
- **Multimodal:** Established a unified modality ingestion pipeline to map distinct data types into a shared cognitive space.

### Phase 5 â€” Scaling & Ablation Studies
- **Ablation (`experiments/ablations.py`):** Systematically removed individual cognitive nodes to prove each part's positive contribution to the system.
- **Baselines (`experiments/baselines.py`):** Verified UCGA is structurally competitive with equivalent-sized MLP, LSTM, and Transformer baselines.
- **Scaling (`experiments/run_scaling.py`):** Demonstrated smooth capacity scaling when increasing `state_dim` and `memory_slots`.

### Phase 6 â€” Intelligence Score & Hardware Optimization
- **Memory Efficiency:** Overcame CUDA Out of Memory (OOM) constraints by implementing mixed-precision (AMP) training and 8-step gradient accumulation (maintaining effective batch size of 256+).
- **Metric Tracking:** Introduced a unified **Intelligence Score** metric to evaluate general cognitive capacity globally across tasks, achieving scores of **0.60+**.

### Phase 7 â€” Memory Architecture Extension
- **Hierarchical Storage:** Implemented `hierarchical_memory.py` to organize knowledge across multiple abstraction levels.
- **Episodic Context:** Added `episodic_memory.py` for short-term scenario-scoped context caching.
- **Attention Overhaul:** Enhanced the `PersistentMemory` with a learned `read_query` projector and a Least-Used-Slot write policy governed by usage counters.

### Phase 8 â€” Visual Builder & Landing Page UI
- **Interactive Visualizer (`landing-page/`):** Built a React + React Flow application providing a visual representation of the cognitive graph.
- **Dynamic Animations:** Developed an animated SVG component that shows pulsing multi-node activation and dynamic data flow particles representing internal cognition.
- **Professional UX:** Shipped a modern UI with Figma-style infinite inertia panning, glassmorphism UI elements, dark mode, and dashboards for internal services (Weaver, Drift-Guard).

### Phase 9 â€” Multi-Agent Scaffolding
- **Agent Ecosystem:** Created the `agents/` directory to manage multi-agent orchestration.
- **Integration:** Hooked the UCGA cognitive model into high-level routing, lifecycle management, and RL (PPO) environments.

---

## ðŸ§  Architecture & Mathematical Formulation

UCGA models intelligence as a 5-tuple: $\mathcal{G} = (\mathcal{V},\; \mathcal{E},\; \mathbf{W},\; \mathcal{S},\; \mathbf{M})$

- $\mathcal{V}$: Intelligence layers (Perception, Cognitive Graph, Working Memory, Reasoning, Global Workspace, Executive Controller, Action).
- $\mathcal{E}$: Directed edges encoding information flow.
- $\mathbf{W}$: Learnable weight matrices.
- $\mathcal{S}$: Internal cognitive state vectors.
- $\mathbf{M}$: Persistent external memory bank.

### AGI Cognitive Cycle

```text
1. Perceive environment
2. Update working memory
3. Activate cognitive graph
4. Perform reasoning
5. Generate goals
6. Plan actions
7. Execute actions
8. Learn and update knowledge graph
```

### Core Equations

**State Update:**
$$v_i^{(t+1)} = \mathrm{LayerNorm}\!\Bigl(\mathrm{GELU}\bigl(W_i \cdot \sum_{j \in \mathcal{N}(i)} v_j^{(t)} + b_i\bigr)\Bigr)$$

**Memory Retrieval (Attention):**
$$\alpha_k = \mathrm{softmax}\!\Bigl(\frac{Q \cdot K_k^\top}{\sqrt{d}}\Bigr), \quad r = \sum_k \alpha_k V_k$$

**Evaluation Confidence:**
$$c = \sigma\!\bigl(W_c \cdot h_{\text{eval}} + b_c\bigr) \in [0, 1]$$

**Conditional Gated Correction:**
$$g = \sigma(W_g \cdot [h_{\text{plan}}; h_{\text{eval}}])$$
$$h_{\text{corrected}} = g \odot f(h_{\text{plan}}, h_{\text{eval}}) + (1 - g) \odot h_{\text{plan}}$$

---

## ðŸ”¬ Experimental Validation

We rigorously validate UCGA on both synthetic cognitive tasks and real-world benchmarks:

| Task / Domain | Metric | Result Highlight |
|------|--------|------------|
| **Polynomial Regression** | MSE | `T=3` significantly outperforms `T=1` (recursion utility confirmed) |
| **Sorting (8 items)** | MSE | `T=3` improves structural re-ordering over single-pass |
| **bAbI (Multi-hop Reasoning)** | Accuracy | Robustly resolves complex multi-hop facts via persistent memory |
| **16-bit Parity (XOR)** | Accuracy | Transformer Reasoning node solves parity while MLP fails |
| **CIFAR-10** | Accuracy | **60.0%** (Validation of CNN Image Encoder integration) |
| **AG News** | Accuracy | **90.5%** (Validation of Text Encoder modeling capability) |

---

## ðŸ“‚ Project Structure

```text
UCGA/
â”œâ”€â”€ README.md                          # This file
â”œâ”€â”€ paper/                             # arXiv-ready research paper
â”‚   â””â”€â”€ main.tex
â”œâ”€â”€ ucga/                              # Core architecture implementation
â”‚   â”œâ”€â”€ nodes/                         # Cognitive processing nodes (Perception, Reasoning, etc.)
â”‚   â”œâ”€â”€ memory/                        # Persistent, working, and episodic memory hierarchy
â”‚   â”œâ”€â”€ encoders/                      # Pluggable Text, Image, Audio, Vector encoders
â”‚   â”œâ”€â”€ distributed_graph.py           # Distributed message passing graph
â”‚   â”œâ”€â”€ lifelong_learner.py            # Elastic Weight Consolidation (EWC)
â”‚   â”œâ”€â”€ adaptive_topology.py           # Self-modifying structure
â”‚   â””â”€â”€ ucga_model.py                  # Main architectural orchestrator
â”œâ”€â”€ training/                          # Optimised scaling & curriculum training scripts
â”œâ”€â”€ experiments/                       # Validation scripts (ROS, LOS, GIB)
â”œâ”€â”€ landing-page/                      # Animated React visual builder & UI
â”œâ”€â”€ agents/                            # PPO RL agents and cognitive environments
â””â”€â”€ utils/                             # Intelligence dashboard, metrics, and loggers
```

---

## âš¡ Quick Start

### Installation
```bash
git clone https://github.com/your-username/UCGA.git
cd UCGA
pip install -r requirements.txt
```

### Run Experiments
Test the breakthrough Transformer vs MLP reasoning hypothesis:
```bash
python experiment_transformer_vs_mlp.py
```
Test temporal recurrence on logical Parity tasks:
```bash
python experiment_phase3_temporal.py
```

### Training & Validation
To run the current optimized scaling phase:
```bash
python training/train_optimized.py
```
To validate intelligence metrics:
```bash
python utils/intelligence_dashboard.py
```

### Launch the Animated Visualizer
Experience the cognitive flow in real-time:
```bash
cd landing-page
npm install
npm run dev
```

---

## âœ… Implementation Status

| Capability | Status |
|-----------|--------|
| Perception | âœ… Complete |
| Memory | âœ… Complete |
| Reasoning (MLP & Transformer) | âœ… Complete |
| Planning | âœ… Complete |
| Executive Control | âœ… Implemented |
| Action System | âœ… Implemented |
| Self-learning & Correction | âœ… Implemented |
| Persistent Memory (Attention-based) | âœ… Complete |
| Hierarchical & Episodic Memory | âœ… Complete |
| Encoders (Vision, Text, Audio, Multimodal) | âœ… Complete |
| UCGAModel core orchestrator | âœ… Complete |
| RL Agent (PPO) & GridWorldEnv | âœ… Complete |
| Intelligence Dashboards | âœ… Complete |
| Training Hardware Optimization (AMP/Accum.) | âœ… Complete |
| UI Visualizer (React + Tailwind) | âœ… Complete |
| Research Paper | âœ… Draft |

---

## ðŸ”® Future Work and Roadmap

UCGA is transitioning from a Cognitive Architecture Prototype to an AGI-capable System. Next milestones:
- Autonomous goal generation and multi-step planning.
- Continuous self-learning and knowledge expansion.
- Learned end-to-end dynamic encoding.
- Dynamic Graph Topology allowing nodes and connections to natively grow based on task demands.
- Large-Scale Benchmarks expanding to ImageNet, full text corpora via multi-node distributed training.

---

## ðŸ“œ Citation & License

If you use UCGA in your research, please cite:

```bibtex
@article{singh2026ucga,
  title={Unified Cognitive Graph Architecture: A Novel Graph-Based Framework
         for Artificial General Intelligence},
  author={Singh, Aman},
  year={2026},
  note={Preprint}
}
```

MIT License â€” Copyright (c) 2026 Aman Singh
See [LICENSE](LICENSE) for details.
